---
title: "Machine Learning Project"
author: "Chase, Ayushi, Rodrigo"
date: "4/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names
library(formattable)

#data
library(ISLR) #for data
library(moderndive) #for data
library(rattle) #weather data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests
```

```{r}
#read in the data
wine <- read_csv("winequality-red.csv")
```




Research question:
How to determine which physiochemical properties make wine 'good'?

Splitting the data into test and training data
```{r}
set.seed(327) 

#70% for training, 30% for test
wine_split <- initial_split(wine, 
                               prop = .7)
wine_train <- training(wine_split)
wine_test <- testing(wine_split)
```

```{r}
mod_rec <- 
  recipe(quality ~., data = wine_train)
```

```{r}
final_wine_train <- mod_rec %>%
  prep() %>%
  bake(new_data = wine_train)

final_wine_test <- mod_rec %>% 
  prep() %>% 
  bake(new_data = wine_test)
```




Taking the data out for a walk:
```{r}
graph1 <- wine_train%>%
  ggplot(aes(x = quality))+
  geom_bar()

graph2 <- wine_train%>%
  ggplot(aes(x = `fixed acidity`, y=density))+
  geom_point()

graph3 <- wine_train%>%
  ggplot(aes(x=density, y=pH))+
  geom_point()

graph4 <- wine_train%>%
  ggplot(aes(x=`fixed acidity`, y=pH))+
  geom_point()

graph5 <- wine_train%>%
  ggplot(aes(x=`free sulfur dioxide`, y=`total sulfur dioxide`))+
  geom_point()

graph6 <- wine_train%>%
  ggplot(aes(x=sulphates, y=alcohol))+
  geom_point()

graph7 <- wine_train%>%
  ggplot(aes(x=`residual sugar`, y=density))+
  geom_point()

graph8 <- wine_train%>%
  ggplot(aes(x=chlorides, y=density))+
  geom_point()

graph9 <- wine_train%>%
  ggplot(aes(x=alcohol, y=chlorides))+
  geom_point()

grid.arrange(graph1, graph2, graph3, nrow=1)
grid.arrange(graph4, graph5, graph6, nrow=2)
grid.arrange(graph7, graph8, graph9, nrow=3)



```

#Fitting a KNN Model
```{r}
knn_rec <- recipe(quality ~ ., data = wine_train) %>%
  
  
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_dummy(all_nominal())

set.seed(253)
wine_knn <- train(
  knn_rec, 
  data = wine_train,
  method = "knn", 
  trControl = trainControl(method = "cv", number = 5), 
  tuneGrid = data.frame(k = c(5, 10,50, 100, 200, 300)))

```

#Results with Optimal K Value = 50
```{r}
wine_knn$results

wine_knn$bestTune$k

wine_knn%>%
  ggplot() +
  geom_line(aes(x=k, y=RMSE), color="blue") 
```

#With the best k value from above, I compute the RMSE Value (remove this: predicted quality) for the wine_test data

```{r}
knn_50 <- train(
  knn_rec, 
  data = wine_train,
  method = "knn",
  tuneGrid = data.frame(k = 50),
  trControl = trainControl(method = "cv", number = 5),
  na.action = na.omit)


#i do not really understand this(i got this from previous HW's)
wine_test <-wine_test %>% 
  mutate(yhat_50 = predict(knn_50, newdata=wine_test))
knn_50$results


```

#Test RMSE for the best Model

```{r}
RMSE_knn<-final_wine_test %>% 
  mutate(pred_quality = predict(knn_50, newdata = wine_test)) %>% 
  summarize(RMSE = sqrt(mean((quality) - (pred_quality))^2))#esther does not have exp 


RMSE_knn
```








```{r}
set.seed(253)
wine_lasso <- train(
  quality ~ .,
  data = wine_train, 
  method = "glmnet",
  trControl = trainControl(method = "cv", 
                           number = 5),
  tuneGrid = data.frame(alpha = 1, 
                        lambda = 10^seq(-3, -1, length = 50)),
  na.action = na.omit)
```

```{r}
wine_lasso$results

wine_lasso$bestTune$lambda

wine_lasso%>%
  ggplot() +
  geom_line(aes(x=lambda, y=RMSE), color="blue") 
```


RMSE for best Model

```{r}
set.seed(253)
wine_lasso_best <- train(
  quality ~ .,
  data = wine_train, 
  method = "glmnet",
  trControl = trainControl(method = "cv", 
                           number = 5),
  tuneGrid = data.frame(alpha = 1, 
                        lambda = 0.001),
  na.action = na.omit)

#i do not really understand this(i got this from previous HW's)
wine_test <-wine_test %>% 
  mutate(yhat_0.001 = predict(wine_lasso_best, newdata=wine_test))
wine_lasso_best$results


coefficients(wine_lasso_best$finalModel, 0.001)



```

Test RMSE for best Model

```{r}

RMSE_lasso<-final_wine_test %>% 
  mutate(pred_quality = predict(wine_lasso_best, newdata = wine_test)) %>% 
  summarize(RMSE = sqrt(mean(((quality) - (pred_quality))^2)))

RMSE_lasso


```


```{r}

RMSE_ALL_MODELS <-data.frame(RMSE_lasso, RMSE_knn)


RMSE_ALL_MODELS<- RMSE_ALL_MODELS%>%
  rename(
    `Lasso RMSE` =RMSE,
    `KNN RMSE` =RMSE.1)


formattable(RMSE_ALL_MODELS)


```


# Linear Model
```{r}
set.seed(86)

linear_model <- lm(quality ~ .,
                   data = wine_train)

resid1 <-
  augment(linear_model) %>% 
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(size = .5, alpha = .5) +
  geom_smooth(se = FALSE) +
  geom_hline(yintercept = 0, color="red")

resid2 <-
  augment(linear_model) %>% 
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()

grid.arrange(resid1, resid2, nrow = 1)  
```

# Logistic Model
```{r}
set.seed(86)

log_model <- lm(log(quality) ~ .,
                data = wine_train)

log_resid1 <-
  augment(log_model) %>% 
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(size = .5, alpha = .5) +
  geom_smooth(se = FALSE) +
  geom_hline(yintercept = 0, color="red")

log_resid2 <-
  augment(log_model) %>% 
  ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line()

grid.arrange(log_resid1, log_resid2, nrow = 1)  
```

# Random Forest
```{r}
set.seed(86)

wine_randf <- train(
  quality ~ .,
  data = wine_train,
  method = "rf",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = 11),
  ntree = 100,
  nodesize = 5,
  na.action = na.omit
)
```

```{r}
wine_randf$results
```
